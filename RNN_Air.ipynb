{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Long Short-Term Memory or LSTM network is a recurrent neural network that is trained using Backpropagation Through Time and overcomes the vanishing gradient problem.\n",
    "\n",
    "As such it can be used to create large recurrent networks, that in turn can be used to address difficult sequence problems in machine learning and achieve state-of-the-art results.\n",
    "\n",
    "Instead of neurons, LSTM networks have memory blocks that are connected into layers.\n",
    "\n",
    "A block has components that make it smarter than a classical neuron and a memory for recent sequences. A block contains gates that manage the block’s state and output. A block operates upon an input sequence and each gate within a block uses the sigmoid activation units to control whether they are triggered or not, making the change of state and addition of information flowing through the block conditional.\n",
    "\n",
    "There are three types of gates within a unit:\n",
    "\n",
    "    Forget Gate: conditionally decides what information to throw away from the block.\n",
    "    Input Gate: conditionally decides which values from the input to update the memory state.\n",
    "    Output Gate: conditionally decides what to output based on input and the memory of the block.\n",
    "\n",
    "Each unit is like a mini-state machine where the gates of the units have weights that are learned during the training procedure.\n",
    "\n",
    "You can see how you may achieve a sophisticated learning and memory from a layer of LSTMs, and it is not hard to imagine how higher-order abstractions may be layered with multiple such layers.\n",
    "LSTM Network For Regression\n",
    "\n",
    "We can phrase the problem as a regression problem.\n",
    "\n",
    "That is, given the number of passengers (in units of thousands) this month, what is the number of passengers next month.\n",
    "\n",
    "We can write a simple function to convert our single column of data into a two-column dataset. The first column containing this month’s (t) passenger count and the second column containing next month’s (t+1) passenger count, to be predicted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "dataset = pandas.read_csv('international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "plt.plot(dataset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, let’s first import all of the functions and classes we intend to use. This assumes a working SciPy environment with the Keras deep learning library installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do anything, it is a good idea to fix the random number seed to ensure our results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the code from the previous section to load the dataset as a Pandas dataframe. We can then extract the NumPy array from the dataframe and convert the integer values to floating point values which are more suitable for modeling with a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataframe = pandas.read_csv('international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs are sensitive to the scale of the input data, specifically when the sigmoid (default) or tanh activation functions are used. It can be a good practice to rescale the data to the range of 0-to-1, also called normalizing. We can easily normalize the dataset using the MinMaxScaler preprocessing class from the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01544401]\n",
      " [ 0.02702703]\n",
      " [ 0.05405405]\n",
      " [ 0.04826255]\n",
      " [ 0.03281853]\n",
      " [ 0.05984557]\n",
      " [ 0.08494207]\n",
      " [ 0.08494207]\n",
      " [ 0.06177607]\n",
      " [ 0.02895753]\n",
      " [ 0.        ]\n",
      " [ 0.02702703]\n",
      " [ 0.02123553]\n",
      " [ 0.04247104]\n",
      " [ 0.07142857]\n",
      " [ 0.05984557]\n",
      " [ 0.04054055]\n",
      " [ 0.08687258]\n",
      " [ 0.12741312]\n",
      " [ 0.12741312]\n",
      " [ 0.10424709]\n",
      " [ 0.05598456]\n",
      " [ 0.01930502]\n",
      " [ 0.06949806]\n",
      " [ 0.07915059]\n",
      " [ 0.08880308]\n",
      " [ 0.14285713]\n",
      " [ 0.11389962]\n",
      " [ 0.13127413]\n",
      " [ 0.14285713]\n",
      " [ 0.18339768]\n",
      " [ 0.18339768]\n",
      " [ 0.15444016]\n",
      " [ 0.11196911]\n",
      " [ 0.08108109]\n",
      " [ 0.1196911 ]\n",
      " [ 0.12934363]\n",
      " [ 0.14671814]\n",
      " [ 0.17181468]\n",
      " [ 0.14864865]\n",
      " [ 0.15250966]\n",
      " [ 0.22007722]\n",
      " [ 0.24324325]\n",
      " [ 0.26640925]\n",
      " [ 0.2027027 ]\n",
      " [ 0.16795367]\n",
      " [ 0.13127413]\n",
      " [ 0.17374519]\n",
      " [ 0.17760617]\n",
      " [ 0.17760617]\n",
      " [ 0.25482625]\n",
      " [ 0.25289574]\n",
      " [ 0.24131274]\n",
      " [ 0.26833975]\n",
      " [ 0.3088803 ]\n",
      " [ 0.32432434]\n",
      " [ 0.25675675]\n",
      " [ 0.20656371]\n",
      " [ 0.14671814]\n",
      " [ 0.18725869]\n",
      " [ 0.19305018]\n",
      " [ 0.16216215]\n",
      " [ 0.25289574]\n",
      " [ 0.23745173]\n",
      " [ 0.25096524]\n",
      " [ 0.3088803 ]\n",
      " [ 0.38223937]\n",
      " [ 0.36486486]\n",
      " [ 0.2992278 ]\n",
      " [ 0.24131274]\n",
      " [ 0.1911197 ]\n",
      " [ 0.24131274]\n",
      " [ 0.26640925]\n",
      " [ 0.24903473]\n",
      " [ 0.31467178]\n",
      " [ 0.31853279]\n",
      " [ 0.32046333]\n",
      " [ 0.40733591]\n",
      " [ 0.50193048]\n",
      " [ 0.46911195]\n",
      " [ 0.40154442]\n",
      " [ 0.32818535]\n",
      " [ 0.25675675]\n",
      " [ 0.33590731]\n",
      " [ 0.34749034]\n",
      " [ 0.33397684]\n",
      " [ 0.41119692]\n",
      " [ 0.4034749 ]\n",
      " [ 0.41312739]\n",
      " [ 0.52123547]\n",
      " [ 0.59652507]\n",
      " [ 0.58108103]\n",
      " [ 0.48455599]\n",
      " [ 0.38996139]\n",
      " [ 0.3223938 ]\n",
      " [ 0.38996139]\n",
      " [ 0.40733591]\n",
      " [ 0.3803089 ]\n",
      " [ 0.48648646]\n",
      " [ 0.47104248]\n",
      " [ 0.48455599]\n",
      " [ 0.61389959]\n",
      " [ 0.69691122]\n",
      " [ 0.70077217]\n",
      " [ 0.57915056]\n",
      " [ 0.46911195]\n",
      " [ 0.38803086]\n",
      " [ 0.44787642]\n",
      " [ 0.45559844]\n",
      " [ 0.41312739]\n",
      " [ 0.49806949]\n",
      " [ 0.47104248]\n",
      " [ 0.49999997]\n",
      " [ 0.63899612]\n",
      " [ 0.74710429]\n",
      " [ 0.7741313 ]\n",
      " [ 0.57915056]\n",
      " [ 0.49227801]\n",
      " [ 0.39768341]\n",
      " [ 0.44980696]\n",
      " [ 0.49420848]\n",
      " [ 0.45945945]\n",
      " [ 0.58301163]\n",
      " [ 0.56370652]\n",
      " [ 0.61003864]\n",
      " [ 0.71042466]\n",
      " [ 0.85714293]\n",
      " [ 0.87837839]\n",
      " [ 0.69305015]\n",
      " [ 0.5849421 ]\n",
      " [ 0.49806949]\n",
      " [ 0.58108103]\n",
      " [ 0.60424709]\n",
      " [ 0.55405402]\n",
      " [ 0.60810804]\n",
      " [ 0.6891892 ]\n",
      " [ 0.71042466]\n",
      " [ 0.83204639]\n",
      " [ 1.        ]\n",
      " [ 0.96911204]\n",
      " [ 0.77992272]\n",
      " [ 0.6891892 ]\n",
      " [ 0.55212355]\n",
      " [ 0.63320458]]\n"
     ]
    }
   ],
   "source": [
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we model our data and estimate the skill of our model on the training dataset, we need to get an idea of the skill of the model on new unseen data. For a normal classification or regression problem we would do this using cross validation.\n",
    "\n",
    "With time series data, the sequence of values is important. A simple method that we can use is to split the ordered dataset into train and test datasets. The code below calculates the index of the split point and separates the data into the training datasets with 67% of the observations that we can use to train our model, leaving the remaining 33% for testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 48)\n",
      "[ 0.01544401  0.02702703]\n",
      "0.0540541\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))\n",
    "print(dataset[0:2,0])\n",
    "print(dataset[2,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a function to create a new dataset as described above.\n",
    "\n",
    "The function takes two arguments, the dataset which is a NumPy array that we want to convert into a dataset and the look_back which is the number of previous time steps to use as input variables to predict the next time period, in this case, defaulted to 1.\n",
    "\n",
    "This default will create a dataset where X is the number of passengers at a given time (t) and Y is the number of passengers at the next time (t + 1).\n",
    "\n",
    "It can be configured and we will by constructing a differently shaped dataset in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare these first 5 rows to the original dataset sample listed in the previous section, you can see the X=t and Y=t+1 pattern in the numbers.\n",
    "\n",
    "Let’s use this function to prepare the train and test datasets ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape into X=t and Y=t+1\n",
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "trainX.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM network expects the input data (X) to be provided with a specific array structure in the form of: [samples, time steps, features].\n",
    "\n",
    "Currently, our data is in the form: [samples, features] and we are framing the problem as one time step for each sample. We can transform the prepared train and test input data into the expected structure using numpy.reshape() as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 1, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "trainX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to design and fit our LSTM network for this problem.\n",
    "\n",
    "The network has a visible layer with 1 input, a hidden layer with 4 LSTM blocks or neurons and an output layer that makes a single value prediction. The default sigmoid activation function is used for the LSTM blocks. The network is trained for 100 epochs and a batch size of 1 is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "0s - loss: 0.0524\n",
      "Epoch 2/100\n",
      "0s - loss: 0.0248\n",
      "Epoch 3/100\n",
      "0s - loss: 0.0164\n",
      "Epoch 4/100\n",
      "0s - loss: 0.0140\n",
      "Epoch 5/100\n",
      "0s - loss: 0.0128\n",
      "Epoch 6/100\n",
      "0s - loss: 0.0117\n",
      "Epoch 7/100\n",
      "0s - loss: 0.0107\n",
      "Epoch 8/100\n",
      "0s - loss: 0.0097\n",
      "Epoch 9/100\n",
      "0s - loss: 0.0088\n",
      "Epoch 10/100\n",
      "0s - loss: 0.0079\n",
      "Epoch 11/100\n",
      "0s - loss: 0.0072\n",
      "Epoch 12/100\n",
      "0s - loss: 0.0064\n",
      "Epoch 13/100\n",
      "0s - loss: 0.0059\n",
      "Epoch 14/100\n",
      "0s - loss: 0.0055\n",
      "Epoch 15/100\n",
      "0s - loss: 0.0051\n",
      "Epoch 16/100\n",
      "0s - loss: 0.0048\n",
      "Epoch 17/100\n",
      "0s - loss: 0.0045\n",
      "Epoch 18/100\n",
      "0s - loss: 0.0045\n",
      "Epoch 19/100\n",
      "0s - loss: 0.0042\n",
      "Epoch 20/100\n",
      "0s - loss: 0.0041\n",
      "Epoch 21/100\n",
      "0s - loss: 0.0040\n",
      "Epoch 22/100\n",
      "0s - loss: 0.0040\n",
      "Epoch 23/100\n",
      "0s - loss: 0.0039\n",
      "Epoch 24/100\n",
      "0s - loss: 0.0039\n",
      "Epoch 25/100\n",
      "0s - loss: 0.0038\n",
      "Epoch 26/100\n",
      "0s - loss: 0.0037\n",
      "Epoch 27/100\n",
      "0s - loss: 0.0037\n",
      "Epoch 28/100\n",
      "0s - loss: 0.0036\n",
      "Epoch 29/100\n",
      "0s - loss: 0.0036\n",
      "Epoch 30/100\n",
      "0s - loss: 0.0036\n",
      "Epoch 31/100\n",
      "0s - loss: 0.0035\n",
      "Epoch 32/100\n",
      "0s - loss: 0.0035\n",
      "Epoch 33/100\n",
      "0s - loss: 0.0034\n",
      "Epoch 34/100\n",
      "0s - loss: 0.0033\n",
      "Epoch 35/100\n",
      "0s - loss: 0.0033\n",
      "Epoch 36/100\n",
      "0s - loss: 0.0033\n",
      "Epoch 37/100\n",
      "0s - loss: 0.0033\n",
      "Epoch 38/100\n",
      "0s - loss: 0.0032\n",
      "Epoch 39/100\n",
      "0s - loss: 0.0032\n",
      "Epoch 40/100\n",
      "0s - loss: 0.0031\n",
      "Epoch 41/100\n",
      "0s - loss: 0.0032\n",
      "Epoch 42/100\n",
      "0s - loss: 0.0031\n",
      "Epoch 43/100\n",
      "0s - loss: 0.0031\n",
      "Epoch 44/100\n",
      "0s - loss: 0.0031\n",
      "Epoch 45/100\n",
      "0s - loss: 0.0030\n",
      "Epoch 46/100\n",
      "0s - loss: 0.0030\n",
      "Epoch 47/100\n",
      "0s - loss: 0.0029\n",
      "Epoch 48/100\n",
      "0s - loss: 0.0030\n",
      "Epoch 49/100\n",
      "0s - loss: 0.0029\n",
      "Epoch 50/100\n",
      "0s - loss: 0.0029\n",
      "Epoch 51/100\n",
      "0s - loss: 0.0029\n",
      "Epoch 52/100\n",
      "0s - loss: 0.0029\n",
      "Epoch 53/100\n",
      "0s - loss: 0.0028\n",
      "Epoch 54/100\n",
      "0s - loss: 0.0028\n",
      "Epoch 55/100\n",
      "0s - loss: 0.0028\n",
      "Epoch 56/100\n",
      "0s - loss: 0.0028\n",
      "Epoch 57/100\n",
      "0s - loss: 0.0028\n",
      "Epoch 58/100\n",
      "0s - loss: 0.0028\n",
      "Epoch 59/100\n",
      "0s - loss: 0.0028\n",
      "Epoch 60/100\n",
      "0s - loss: 0.0027\n",
      "Epoch 61/100\n",
      "0s - loss: 0.0027\n",
      "Epoch 62/100\n",
      "0s - loss: 0.0028\n",
      "Epoch 63/100\n",
      "0s - loss: 0.0027\n",
      "Epoch 64/100\n",
      "0s - loss: 0.0027\n",
      "Epoch 65/100\n",
      "0s - loss: 0.0026\n",
      "Epoch 66/100\n",
      "0s - loss: 0.0026\n",
      "Epoch 67/100\n",
      "0s - loss: 0.0026\n",
      "Epoch 68/100\n",
      "0s - loss: 0.0026\n",
      "Epoch 69/100\n",
      "0s - loss: 0.0026\n",
      "Epoch 70/100\n",
      "0s - loss: 0.0026\n",
      "Epoch 71/100\n",
      "0s - loss: 0.0026\n",
      "Epoch 72/100\n",
      "0s - loss: 0.0026\n",
      "Epoch 73/100\n",
      "0s - loss: 0.0026\n",
      "Epoch 74/100\n",
      "0s - loss: 0.0026\n",
      "Epoch 75/100\n",
      "0s - loss: 0.0025\n",
      "Epoch 76/100\n",
      "0s - loss: 0.0026\n",
      "Epoch 77/100\n",
      "0s - loss: 0.0025\n",
      "Epoch 78/100\n",
      "0s - loss: 0.0025\n",
      "Epoch 79/100\n",
      "0s - loss: 0.0025\n",
      "Epoch 80/100\n",
      "0s - loss: 0.0025\n",
      "Epoch 81/100\n",
      "0s - loss: 0.0025\n",
      "Epoch 82/100\n",
      "0s - loss: 0.0025\n",
      "Epoch 83/100\n",
      "0s - loss: 0.0025\n",
      "Epoch 84/100\n",
      "0s - loss: 0.0025\n",
      "Epoch 85/100\n",
      "0s - loss: 0.0024\n",
      "Epoch 86/100\n",
      "0s - loss: 0.0024\n",
      "Epoch 87/100\n",
      "0s - loss: 0.0024\n",
      "Epoch 88/100\n",
      "0s - loss: 0.0024\n",
      "Epoch 89/100\n",
      "0s - loss: 0.0024\n",
      "Epoch 90/100\n",
      "0s - loss: 0.0024\n",
      "Epoch 91/100\n",
      "0s - loss: 0.0024\n",
      "Epoch 92/100\n",
      "0s - loss: 0.0024\n",
      "Epoch 93/100\n",
      "0s - loss: 0.0024\n",
      "Epoch 94/100\n",
      "0s - loss: 0.0025\n",
      "Epoch 95/100\n",
      "0s - loss: 0.0024\n",
      "Epoch 96/100\n",
      "0s - loss: 0.0024\n",
      "Epoch 97/100\n",
      "0s - loss: 0.0025\n",
      "Epoch 98/100\n",
      "0s - loss: 0.0024\n",
      "Epoch 99/100\n",
      "0s - loss: 0.0024\n",
      "Epoch 100/100\n",
      "0s - loss: 0.0023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcbca6c5050>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_dim=look_back))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, nb_epoch=100, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.010638 using {'nb_epoch': 100, 'neurons': 2, 'batch_size': 3}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 2, 'batch_size': 1}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 3, 'batch_size': 1}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 4, 'batch_size': 1}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 5, 'batch_size': 1}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 2, 'batch_size': 2}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 3, 'batch_size': 2}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 4, 'batch_size': 2}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 5, 'batch_size': 2}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 2, 'batch_size': 3}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 3, 'batch_size': 3}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 4, 'batch_size': 3}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 5, 'batch_size': 3}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 2, 'batch_size': 4}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 3, 'batch_size': 4}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 4, 'batch_size': 4}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 5, 'batch_size': 4}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 2, 'batch_size': 5}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 3, 'batch_size': 5}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 4, 'batch_size': 5}\n",
      "0.010417 (0.014731) with: {'nb_epoch': 100, 'neurons': 5, 'batch_size': 5}\n",
      "100\n",
      "2\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Grid search parameters\n",
    "\n",
    "def create_model(neurons=1):\n",
    "\n",
    "\t# create and fit the LSTM network\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(neurons, input_dim=look_back))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam',  metrics=['accuracy'])\n",
    "\t#model.fit(trainX, trainY, nb_epoch=100, batch_size=1, verbose=2)\n",
    "\treturn model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [1, 2, 3, 4, 5]\n",
    "epochs = [100]\n",
    "neurons = [2, 3, 4, 5]\n",
    "\n",
    "\n",
    "param_grid = dict(neurons=neurons,batch_size=batch_size, nb_epoch=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(trainX, trainY)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "for params, mean_score, scores in grid_result.grid_scores_:\n",
    "    print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))\n",
    "print(grid_result.best_params_['nb_epoch'])\t\n",
    "print(grid_result.best_params_['neurons'])\n",
    "print(grid_result.best_params_['batch_size'])\n",
    "print(look_back)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is fit, we can estimate the performance of the model on the train and test datasets. This will give us a point of comparison for new models.\n",
    "\n",
    "Note that we invert the normalization using the same scaler object to get mean errors in squared units (thousands of passengers per month)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 129.53 RMSE\n",
      "Test Score: 182.27 RMSE\n"
     ]
    }
   ],
   "source": [
    "# Estimate model performance\n",
    "trainScore = model.evaluate(trainX, trainY, verbose=0)\n",
    "trainScore = math.sqrt(trainScore)\n",
    "trainScore = scaler.inverse_transform(numpy.array([[trainScore]]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "\n",
    "testScore = model.evaluate(testX, testY, verbose=0)\n",
    "testScore = math.sqrt(testScore)\n",
    "testScore = scaler.inverse_transform(numpy.array([[testScore]]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can generate predictions using the model for both the train and test dataset to get a visual indication of the skill of the model.\n",
    "\n",
    "Because of how the dataset was prepared, we must shift the predictions so that they aline on the x-axis with the original dataset. Once prepared, the data is plotted, showing the original dataset in blue, the predictions for the train dataset in green and the predictions on the unseen test dataset in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# generate predictions for training\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(dataset)\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
